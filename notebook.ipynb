{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import wget\n",
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "\n",
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "from langchain.agents import load_tools, Tool\n",
    "from langchain.utilities.google_search import GoogleSearchAPIWrapper\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from random import sample\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from langchain.chains import LLMRequestsChain, LLMChain\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "\n",
    "# First, let's load the language model we're going to use to control the agent.\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "\n",
    "google_search = GoogleSearchAPIWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from tqdm.auto import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def select_snippets_with_keywords(keyword: str, text: str, window_size: int = 500):\n",
    "    # Window size is in characters\n",
    "    all_keyword_indexes = [m.start()\n",
    "                           for m in re.finditer(f\"({keyword}).*[\\\\.]\", text)]\n",
    "    output = []\n",
    "    for index in all_keyword_indexes:\n",
    "        left_window = max(0, index - window_size)\n",
    "        right_window = min(len(text), index + window_size)\n",
    "        snippet = text[left_window:right_window]\n",
    "        output.append(snippet)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def get_keywords(query):\n",
    "    template = \"Select at most 6 most meaningful keywords in lowercase and separated by a comma from this text : {text}\"\n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
    "    get_keywords_chain = LLMChain(\n",
    "        prompt=prompt, llm=OpenAI(temperature=0), verbose=True)\n",
    "\n",
    "    return [k.strip() for k in get_keywords_chain.run(query).split(\",\")]\n",
    "\n",
    "\n",
    "def google_search_about_insurance(query: str) -> str:\n",
    "    keywords = get_keywords(query)\n",
    "    print(keywords)\n",
    "\n",
    "    google_query = \"Cyber attack company financial loss business interruption\"\n",
    "    r = google_search._google_search_results(\n",
    "        google_query\n",
    "    )\n",
    "    if len(r) == 0:\n",
    "        return \"No link\"\n",
    "    else:\n",
    "        all_docs = []\n",
    "        embeddings = HuggingFaceEmbeddings()\n",
    "        vectorstore = None\n",
    "        text_splitter = SpacyTextSplitter.from_tiktoken_encoder(\n",
    "            chunk_size=500, chunk_overlap=0, pipeline=\"en_core_web_sm\"\n",
    "        )\n",
    "\n",
    "        all_relevant_snippets = []\n",
    "\n",
    "        for result in (pbar := tqdm(r[:20])):\n",
    "            result_url = result[\"link\"]  # Take the URL of the first result\n",
    "            pbar.set_description(result_url)\n",
    "\n",
    "            # Download file\n",
    "            html = requests.get(result_url).text\n",
    "            soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "            # kill all script and style elements\n",
    "            for script in soup([\"script\", \"style\"]):\n",
    "                script.extract()    # rip it out\n",
    "\n",
    "            # get text\n",
    "            text = soup.get_text()\n",
    "\n",
    "            # break into lines and remove leading and trailing space on each\n",
    "            lines = (line.strip() for line in text.splitlines())\n",
    "            # break multi-headlines into a line each\n",
    "            chunks = (phrase.strip()\n",
    "                      for line in lines for phrase in line.split(\"  \"))\n",
    "            # drop blank lines\n",
    "            document_text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "            if len(document_text) < 100:\n",
    "                continue\n",
    "\n",
    "            relevant_snippets = []\n",
    "\n",
    "            snippets_with_keyword = []\n",
    "            for keyword in keywords:\n",
    "                snippets_with_keyword = select_snippets_with_keywords(\n",
    "                    keyword, document_text, window_size=300)\n",
    "            snippets_with_keyword = text_splitter.split_text(\n",
    "                \"\\n\".join(snippets_with_keyword))\n",
    "\n",
    "            min_snippets = 1\n",
    "            max_snippets = 5\n",
    "            max_vector_store = 20\n",
    "\n",
    "            if len(snippets_with_keyword) > max_snippets:\n",
    "                if len(snippets_with_keyword) > max_vector_store:\n",
    "                    snippets_with_keyword = sample(\n",
    "                        snippets_with_keyword, max_vector_store)\n",
    "                vectorstore = FAISS.from_texts(\n",
    "                    snippets_with_keyword, embeddings)\n",
    "                relevant_snippets = vectorstore.similarity_search(\n",
    "                    google_query, max_snippets)\n",
    "            elif 0 < len(snippets_with_keyword) < min_snippets:\n",
    "                all_snippets = text_splitter.split_text(document_text)\n",
    "                random_snippets = sample(all_snippets, min(\n",
    "                    max_vector_store - len(snippets_with_keyword), len(all_snippets)))\n",
    "                vectorstore = FAISS.from_texts(\n",
    "                    snippets_with_keyword, embeddings)\n",
    "                relevant_snippets = text_splitter.create_documents(\n",
    "                    snippets_with_keyword)\n",
    "                relevant_snippets += vectorstore.similarity_search(\n",
    "                    google_query, max_snippets - len(snippets_with_keyword))\n",
    "\n",
    "            else:\n",
    "                relevant_snippets = snippets_with_keyword\n",
    "\n",
    "            all_relevant_snippets += relevant_snippets\n",
    "\n",
    "        all_relevant_snippets = [\n",
    "            snip for snip in all_relevant_snippets if isinstance(snip, str)]\n",
    "\n",
    "        # Select the most relevant snippets from the collection\n",
    "        vectorstore = FAISS.from_texts(all_relevant_snippets, embeddings)\n",
    "        docs = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "        # Reply to the question\n",
    "        chain = load_qa_chain(llm, chain_type=\"stuff\", verbose=True)\n",
    "        summary = chain.run(input_documents=docs, question=query)\n",
    "        return summary\n",
    "\n",
    "\n",
    "google_search_insurance = Tool(\n",
    "    \"Google Search about insurance\",\n",
    "    google_search_about_insurance,\n",
    "    \"A wrapper around Google Search.\",\n",
    ")\n",
    "\n",
    "tools = [google_search_insurance]\n",
    "\n",
    "\n",
    "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(\n",
    "    tools, llm, agent=\"zero-shot-react-description\", verbose=True)\n",
    "\n",
    "agent.run(\"\"\"List several historical losses due to business interruption caused by cyber-attacks.\n",
    "For each example, list the year, the company name, the duration of business interruption and the financial cost.\n",
    "For instance: In 2018, an attack at company Saint-Gobain caused 10 days of business interruption, with a financial loss of 700M$.\n",
    "\"\"\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
